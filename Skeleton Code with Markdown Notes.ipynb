{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Import Libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Lets read and focus on one dataframe first\n",
    "stk12_full = \"STK12.csv\"\n",
    "\n",
    "Can explain why we are looking at only one at a time instead of doing them in series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Then concatenate the facid column with the arbid id to create a unique identifier for the stk12 dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) When comparing the stackdata with the emission data, there is a small difference which we will assign to as a ground_level point.\n",
    "Load in the \"stk12_abl.csv\" which has the sum of emissions from each stack by arbid_facid and the \"em12_abl.csv\" which has the sum of emissions by arbid_facid. Now join the 2(summed stacks and emissions) based on \"arbid_facid\" and create 5 series based on the difference between the pm,nox,sox,voc, and nh3 columns. Assign these series under 5 new columns in the joined dataframe.\n",
    "(Let's break up the steps into 4a, 4ai), etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Now grab the unique values from the stk2012 data as a new dataframe and then merge it with the joined dataframe from the previous step. (\"I am not sure how they turned these into different data rows as new \"stacks\"). To distinguish them from the other stacks, assign them \"9999\" for the stk column, \"Computed ground-level\" for the stkname, and NA for the stack height and stack diameter columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) These new ground level sources are missing geographic information so import \"masterdata_1119.csv\" and only in the unique facilities. After that, merge it with the dataframe with the ground-level difference values and rename the lat/long coordinates to \"Lat_WGS84\"/\"Long_WGS84\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) At last, we will rename the lat/long coordinate columns for the original stk_12 dataframe to match the previous dataframes and merge them so we will now have a complete list of stack plus ground level emissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) A quick check-in to make sure everything is correct is to sum all the emissions in the previous data frame and sum all the emissions in the em12_ab1.csv and check that they are equal. Afterward, repeat this same exact process for the 2017 data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) We shall now make other identifiers to keep track of whether ARBID_FACID have changed overtime, if there are more than one FACID assigned to each ARBID, and how many FACIDs are associated with each ARBID. First we shall splice the stk12 data once again by selecting only the \"(arbid_facid, ARBID, FACID, DIS, AB, CO)\" columns and grouping by ARBID(what does this groupby do,maybe count?). Afterward, we will only take the unique arbid_facids and will use this new dataframe for the following steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10) To check if the ARBID_FACID has changed overtime, we must read the \"ton.csv\" file. Then we will make a new column with the concatnation of the facid and arbid(may be listed as FAC_ID). After this, we will only take the unique arbid_facid from the ton dataframe and create a new column titled \"TON\" that is set to TRUE. Then we will splice this ton dataframe, taking only the TON and arbid_facid column. Then we will merge this splice with the dataframe from the step above to make a new dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11) Now to idenfity the issue of multiple FACID corresponding to the same ARBID. Take the dataframe from the previous step and make a new one by grouping by arbid(count) and only grabbing the one with a count greater than 1. Then we will make a new column called \"DUP\" and assign it all to be TRUE. Then we shall merge this dataframe with the one with the previous step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12) Now take the dataframe from the previous step and replace the na's with False for the Ton column, False for the DUP column, and 1 for the Count column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Now merge the dataframe from the previous step with the full datafram from step 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Section\n",
    "## Another New Section\n",
    "\n",
    "This is a **sentence**\n",
    "\n",
    "* bullet number 1\n",
    "\n",
    "When referring to column headers/titles, do it like this: `FACID`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can write a top section about the purpose of this skeleton code. List the input files needed with a brief description on each of them. Write a status of the code where it currently is(date last modified and what needs to be improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
